{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Extraction and Cleaning from 'Project Description' of each Kickstarter Campaign**\n",
    "\n",
    "This notebook has functions for extracting various features from the project description of each campaign.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for text cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(scraped_html):\n",
    "    \"\"\"Use the BeautifulSoup library to parse the scraped HTML of a project \n",
    "    using an lxml parser\"\"\"\n",
    "    \n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "\n",
    "def cleaning(text):    \n",
    "    \n",
    "    # Remove line breaks, leading and trailing whitespace, and compress all\n",
    "    # whitespace to a single space\n",
    "    text_cleaned = ' '.join(text.split()).strip()\n",
    "    \n",
    "    # Remove the HTML5 warning for videos\n",
    "    return text_cleaned.replace(\"You'll need an HTML5 capable browser to see this content. \" + \\\n",
    "        \"Play Replay with sound Play with sound 00:00 00:00\",' ')\n",
    "\n",
    "\n",
    "def campaign_details(soup):\n",
    "    \n",
    "    # Collect the \"About this project\" section if available\n",
    "    try:\n",
    "        section1 = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive-media ' + \\\n",
    "                'formatted-lists'\n",
    "        ).get_text(' ')\n",
    "    except AttributeError:\n",
    "        section1 = 'section_not_found'\n",
    "    \n",
    "    # Collect the \"Risks and challenges\" section if available, and remove all\n",
    "    # unnecessary text\n",
    "    try:\n",
    "        section2 = soup.find(\n",
    "            'div', \n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ) \\\n",
    "            .get_text(' ') \\\n",
    "            .replace('Risks and challenges',' ') \\\n",
    "            .replace('Learn about accountability on Kickstarter',' ')\n",
    "    except AttributeError:\n",
    "        section2 = 'section_not_found'\n",
    "    \n",
    "    # Clean both campaign sections\n",
    "    return {'about': cleaning(section1), 'risks': cleaning(section2)}\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    # Tag email addresses with regex\n",
    "    normalized = re.sub(\n",
    "        r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b',\n",
    "        'emailaddr',\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    # Tag hyperlinks with regex\n",
    "    normalized = re.sub(\n",
    "        r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)',\n",
    "        'httpaddr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag money amounts with regex\n",
    "    normalized = re.sub(r'\\$\\d+(\\.\\d+)?', 'dollramt', normalized)\n",
    "    \n",
    "    # Tag percentages with regex\n",
    "    normalized = re.sub(r'\\d+(\\.\\d+)?\\%', 'percntg', normalized)\n",
    "    \n",
    "    # Tag phone numbers with regex\n",
    "    normalized = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr',\n",
    "        normalized\n",
    "    )\n",
    "    \n",
    "    # Tag remaining numbers with regex\n",
    "    return re.sub(r'\\d+(\\.\\d+)?', 'numbr', normalized)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions for extracting imp features from Campaign Descriptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_token(text):\n",
    "     # Tokenize the text into sentences\n",
    "    return nltk.sent_tokenize(text)\n",
    "\n",
    "\n",
    "def punc_cleaning(text):\n",
    " \n",
    "    # Remove punctuation with regex\n",
    "    return re.sub(r'[^\\w\\d\\s]|\\_', ' ', text)\n",
    "\n",
    "\n",
    "def token_words(text):\n",
    "    \n",
    "    # Remove punctuation and then tokenize the text into words\n",
    "    return [word for word in nltk.word_tokenize(punc_cleaning(text))]\n",
    "\n",
    "\n",
    "def search_allcaps(text):\n",
    "        \n",
    "    # Identify all-caps words with regex\n",
    "    return re.findall(r'\\b[A-Z]{2,}', text)\n",
    "\n",
    "\n",
    "def exclamations_count(text):\n",
    "    \n",
    "    # Count the number of exclamation marks in the text\n",
    "    return text.count('!')\n",
    "\n",
    "\n",
    "def imp_words_count(text):\n",
    "    # Define a set of adjectives used commonly by project writers\n",
    "    imp_words = frozenset(\n",
    "        ['revolutionary', 'breakthrough', 'beautiful', 'magical', \n",
    "        'gorgeous', 'amazing', 'incredible', 'awesome']\n",
    "    )\n",
    "    \n",
    "    # Count total number of imp_words in the text\n",
    "    return sum(1 for word in words_token(text) if word in imp_words)\n",
    "\n",
    "\n",
    "\n",
    "def avg_words_count(text):\n",
    "    \n",
    "    # Compute the average number of words in each sentence\n",
    "    return pd.Series(\n",
    "        [len(words_token(sentence)) for sentence in sentences_token(text)]\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "\n",
    "def paragraphs_count(soup, section):    \n",
    "    \n",
    "    # Use tree parsing to count the number of paragraphs depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p'))\n",
    "    \n",
    "def avg_sents_paragraph(soup, section):\n",
    "    #look at 'about' section\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of sentences in each paragraph    \n",
    "    return pd.Series(\n",
    "        [len(sentences_token(paragraph.get_text(' '))) for paragraph in \\\n",
    "         paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "def avg_words_paragraph(soup, section):\n",
    "\n",
    "    # Use tree parsing to identify all paragraphs depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('p')\n",
    "    elif section == 'risks':\n",
    "        paragraphs = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('p')\n",
    "    \n",
    "    # Compute the average number of words in each paragraph\n",
    "    return pd.Series(\n",
    "        [len(words_token(paragraph.get_text(' '))) for paragraph in paragraphs]\n",
    "    ).mean()\n",
    "\n",
    "def images_count(soup, section):    \n",
    "    \n",
    "    # Use tree parsing to identify all image tags depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('img'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('img'))\n",
    "    \n",
    "def videos_count(soup, section):    \n",
    "    \n",
    "    # Use tree parsing to count all non-YouTube video tags depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('div', class_='video-player'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "         ).find_all('div', class_='video-player'))\n",
    "\n",
    "def youtube_count(soup, section):    \n",
    "    \n",
    "    # Initialize total number of YouTube videos\n",
    "    youtube_count = 0\n",
    "\n",
    "    # Use tree parsing to identify all iframe tags depending on which section\n",
    "    # is requested\n",
    "    if section == 'about':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "            '-media formatted-lists'\n",
    "        ).find_all('iframe')\n",
    "    elif section == 'risks':\n",
    "        iframes = soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('iframe')\n",
    "    \n",
    "    # Since YouTube videos are contained only in iframe tags, determine which\n",
    "    # iframe tags contain YouTube videos and count them\n",
    "    for iframe in iframes:\n",
    "        # Catch any iframes that fail to include a YouTube source link\n",
    "        try:\n",
    "            if 'youtube' in iframe.get('src'):\n",
    "                youtube_count += 1\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    return youtube_count\n",
    "\n",
    "\n",
    "def hyperlinks_count(soup, section):    \n",
    "    \"\"\"Count the number of hyperlink tags in a campaign section\"\"\"\n",
    "    # Use tree parsing to compute number of hyperlink tags depending on the\n",
    "    # section requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('a'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('a'))\n",
    "    \n",
    "def count_bold_tags(soup, section):    \n",
    "    \"\"\"Count the number of bold tags in a campaign section\"\"\"\n",
    "    \n",
    "    # Use tree parsing to compute number of bolded text tags depending on which\n",
    "    # section is requested\n",
    "    if section == 'about':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='full-description js-full-description responsive' + \\\n",
    "                '-media formatted-lists'\n",
    "        ).find_all('b'))\n",
    "    elif section == 'risks':\n",
    "        return len(soup.find(\n",
    "            'div',\n",
    "            class_='mb3 mb10-sm mb3 js-risks'\n",
    "        ).find_all('b'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Perform text preprocessing such as removing punctuation, lowercasing all\n",
    "    words, removing stop words and stemming remaining words\"\"\"\n",
    "    \n",
    "    # Access stop word dictionary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # Initialize the Porter stemmer\n",
    "    porter = nltk.PorterStemmer()\n",
    "    \n",
    "    # Remove punctuation and lowercase each word\n",
    "    text = remove_punc(text).lower()\n",
    "    \n",
    "    # Remove stop words and stem each word\n",
    "    return ' '.join(\n",
    "        porter.stem(term )\n",
    "        for term in text.split()\n",
    "        if term not in set(stop_words)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading html scrapped data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scraped data\n",
    "filename1 = '/Users/shwetapai/Desktop/Project5/Data/scraped_collection_0-4999.pkl'\n",
    "filename2='/Users/shwetapai/Desktop/Project5/Data/scraped_collection_5001-6015.pkl'\n",
    "scraped_data1 = joblib.load(filename1)\n",
    "scarped_data2=joblib.load(filename2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Super function to extract all features from the text of a campaign**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(soup, campaign, section):\n",
    "    \"\"\"Extract all the features of the text of campaign section\"\"\"\n",
    "   \n",
    "    \n",
    "    # Compute the number of words in the section\n",
    "    num_words = len(words_token(campaign[section]))\n",
    "    \n",
    "    # If the section contains no words, assign NaN to 'num_words' to avoid\n",
    "    # potential division by zero\n",
    "    if num_words == 0:\n",
    "        num_words = np.nan\n",
    "        \n",
    "    #If the section isn't available, then return NaN for each meta feature.\n",
    "    if campaign[section] == 'section_not_found':\n",
    "        return([np.nan] * 19)\n",
    "    else:\n",
    "        return (\n",
    "            len(sentences_token(campaign[section])),  #number of the sentence\n",
    "            num_words,                                # number of words\n",
    "            len(search_allcaps(campaign[section])), # number of all_caps\n",
    "            len(search_allcaps(campaign[section])) / num_words,  #% of all caps\n",
    "            exclamations_count(campaign[section]),              #number of exclamations\n",
    "            exclamations_count(campaign[section]) / num_words,    #% of exclamations\n",
    "            imp_words_count(campaign[section]),                   #number of buzz words\n",
    "            imp_words_count(campaign[section]) / num_words,     #% of buzz words\n",
    "            avg_words_count(campaign[section]),                #number of avg words\n",
    "            paragraphs_count(soup, section),                     #number of paragraphs\n",
    "            avg_sents_paragraph(soup, section),          #number of sentences per paragraph\n",
    "            avg_words_paragraph(soup, section),          #number of words per paragraph\n",
    "            images_count(soup, section),                         #number of images\n",
    "            videos_count(soup, section),                        # number of videos\n",
    "            youtube_count(soup, section),                       #number of youtube videos\n",
    "            hyperlinks_count(soup, section),                    #number of hyperlinks\n",
    "            count_bold_tags(soup, section),                      #number of bold tag\n",
    "            count_bold_tags(soup, section) / num_words,          #%of bold tags\n",
    "            campaign[section]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames of features for each section\n",
    "features = ['num_sents', 'num_words', 'num_all_caps', 'percent_all_caps',\n",
    "            'num_exclms', 'percent_exclms', 'num_imp_words',\n",
    "            'percent_imp_words', 'avg_words_per_sent', 'num_paragraphs',\n",
    "            'avg_sents_per_paragraph', 'avg_words_per_paragraph',\n",
    "            'num_images', 'num_videos', 'num_youtubes',\n",
    "            'num_hyperlinks', 'num_bolded', 'percent_bolded',\n",
    "            'normalized_text']\n",
    "df_description = pd.DataFrame(columns=features)\n",
    "\n",
    "df_description1 = pd.DataFrame(columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(scraped_html):\n",
    "    \"\"\"Use the BeautifulSoup library to parse the scraped HTML of a project \n",
    "    using an lxml parser\"\"\"\n",
    "\n",
    "    # Parse the HTML content using an lxml parser\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "\n",
    "#parsing scrapped html data\n",
    "for index, row in scraped_data1.iterrows():\n",
    "    \n",
    "    soup = parse(row[0])\n",
    "\n",
    "    \n",
    "    # Normalize campaign sections ('About and 'Risks')\n",
    "    campaign = campaign_details(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    campaign['risks'] = normalize(campaign['risks'])\n",
    "    \n",
    "    \n",
    "    # Extract meta features for each section\n",
    "    df_description.loc[index] = feature_extraction(soup, campaign, 'about')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(scraped_html):\n",
    "    \"\"\"Use the BeautifulSoup library to parse the scraped HTML of a project \n",
    "    using an lxml parser\"\"\"\n",
    "\n",
    "    # Parse the HTML content using an lxml parser\n",
    "    return BeautifulSoup(scraped_html.text, 'lxml')\n",
    "\n",
    "\n",
    "#parsing scrapped html data\n",
    "for index, row in scarped_data2.iterrows():\n",
    "    \n",
    "    soup = parse(row[0])\n",
    "\n",
    "    \n",
    "    # Normalize campaign sections ('About and 'Risks')\n",
    "    campaign = campaign_details(soup)\n",
    "    campaign['about'] = normalize(campaign['about'])\n",
    "    \n",
    "    \n",
    "    # Extract meta features for each section\n",
    "    df_description1.loc[index] = feature_extraction(soup, campaign, 'about')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_all_caps</th>\n",
       "      <th>percent_all_caps</th>\n",
       "      <th>num_exclms</th>\n",
       "      <th>percent_exclms</th>\n",
       "      <th>num_imp_words</th>\n",
       "      <th>percent_imp_words</th>\n",
       "      <th>avg_words_per_sent</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>avg_sents_per_paragraph</th>\n",
       "      <th>avg_words_per_paragraph</th>\n",
       "      <th>num_images</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>num_youtubes</th>\n",
       "      <th>num_hyperlinks</th>\n",
       "      <th>num_bolded</th>\n",
       "      <th>percent_bolded</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6568</th>\n",
       "      <td>65</td>\n",
       "      <td>1248</td>\n",
       "      <td>24</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>15</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>19.200000</td>\n",
       "      <td>16</td>\n",
       "      <td>2.687500</td>\n",
       "      <td>55.312500</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>We've reached our first stretch goal of dollra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6569</th>\n",
       "      <td>48</td>\n",
       "      <td>916</td>\n",
       "      <td>20</td>\n",
       "      <td>0.021834</td>\n",
       "      <td>11</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>19.083333</td>\n",
       "      <td>17</td>\n",
       "      <td>2.588235</td>\n",
       "      <td>49.235294</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>You can fund the next generation of entreprene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>101</td>\n",
       "      <td>1845</td>\n",
       "      <td>63</td>\n",
       "      <td>0.034146</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.267327</td>\n",
       "      <td>40</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>30.200000</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0.028184</td>\n",
       "      <td>STRETCH GOAL dollramt,numbr. All backers will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6572</th>\n",
       "      <td>34</td>\n",
       "      <td>532</td>\n",
       "      <td>22</td>\n",
       "      <td>0.041353</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.647059</td>\n",
       "      <td>31</td>\n",
       "      <td>1.774194</td>\n",
       "      <td>15.838710</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.041353</td>\n",
       "      <td>Fast percntg Charging (numbrV numbrA Input ) I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6573</th>\n",
       "      <td>32</td>\n",
       "      <td>551</td>\n",
       "      <td>14</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>17.218750</td>\n",
       "      <td>17</td>\n",
       "      <td>2.058824</td>\n",
       "      <td>29.470588</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.014519</td>\n",
       "      <td>The Purpose: The purpose of The Sirens Project...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_sents num_words num_all_caps  percent_all_caps num_exclms  \\\n",
       "6568        65      1248           24          0.019231         15   \n",
       "6569        48       916           20          0.021834         11   \n",
       "6571       101      1845           63          0.034146          0   \n",
       "6572        34       532           22          0.041353          0   \n",
       "6573        32       551           14          0.025408          5   \n",
       "\n",
       "      percent_exclms num_imp_words  percent_imp_words  avg_words_per_sent  \\\n",
       "6568        0.012019             1           0.000801           19.200000   \n",
       "6569        0.012009             1           0.001092           19.083333   \n",
       "6571        0.000000             0           0.000000           18.267327   \n",
       "6572        0.000000             0           0.000000           15.647059   \n",
       "6573        0.009074             1           0.001815           17.218750   \n",
       "\n",
       "     num_paragraphs  avg_sents_per_paragraph  avg_words_per_paragraph  \\\n",
       "6568             16                 2.687500                55.312500   \n",
       "6569             17                 2.588235                49.235294   \n",
       "6571             40                 1.900000                30.200000   \n",
       "6572             31                 1.774194                15.838710   \n",
       "6573             17                 2.058824                29.470588   \n",
       "\n",
       "     num_images num_videos num_youtubes num_hyperlinks num_bolded  \\\n",
       "6568         21          0            0              0         10   \n",
       "6569          7          0            0              5         10   \n",
       "6571         11          1            0              1         52   \n",
       "6572         15          0            0              1         22   \n",
       "6573         13          0            0              7          8   \n",
       "\n",
       "      percent_bolded                                    normalized_text  \n",
       "6568        0.008013  We've reached our first stretch goal of dollra...  \n",
       "6569        0.010917  You can fund the next generation of entreprene...  \n",
       "6571        0.028184  STRETCH GOAL dollramt,numbr. All backers will ...  \n",
       "6572        0.041353  Fast percntg Charging (numbrV numbrA Input ) I...  \n",
       "6573        0.014519  The Purpose: The purpose of The Sirens Project...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_all_caps</th>\n",
       "      <th>percent_all_caps</th>\n",
       "      <th>num_exclms</th>\n",
       "      <th>percent_exclms</th>\n",
       "      <th>num_imp_words</th>\n",
       "      <th>percent_imp_words</th>\n",
       "      <th>avg_words_per_sent</th>\n",
       "      <th>num_paragraphs</th>\n",
       "      <th>avg_sents_per_paragraph</th>\n",
       "      <th>avg_words_per_paragraph</th>\n",
       "      <th>num_images</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>num_youtubes</th>\n",
       "      <th>num_hyperlinks</th>\n",
       "      <th>num_bolded</th>\n",
       "      <th>percent_bolded</th>\n",
       "      <th>normalized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6561</th>\n",
       "      <td>48</td>\n",
       "      <td>833</td>\n",
       "      <td>6</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.354167</td>\n",
       "      <td>16</td>\n",
       "      <td>3.187500</td>\n",
       "      <td>50.812500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>Imagine a world where there are people that ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6562</th>\n",
       "      <td>28</td>\n",
       "      <td>420</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>Synopsis Set in Orlando, Florida, Blood Child ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6563</th>\n",
       "      <td>42</td>\n",
       "      <td>851</td>\n",
       "      <td>21</td>\n",
       "      <td>0.024677</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>20.261905</td>\n",
       "      <td>14</td>\n",
       "      <td>3.285714</td>\n",
       "      <td>58.357143</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>Click the image to read about these books The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6564</th>\n",
       "      <td>10</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>I originally planned to quietly publish this n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>120</td>\n",
       "      <td>2032</td>\n",
       "      <td>17</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>26</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>16.933333</td>\n",
       "      <td>12</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>Thank You! To get in touch with me about my pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_sents num_words num_all_caps  percent_all_caps num_exclms  \\\n",
       "6561        48       833            6          0.007203          7   \n",
       "6562        28       420            0          0.000000          0   \n",
       "6563        42       851           21          0.024677          2   \n",
       "6564        10       149            0          0.000000          0   \n",
       "6566       120      2032           17          0.008366         26   \n",
       "\n",
       "      percent_exclms num_imp_words  percent_imp_words  avg_words_per_sent  \\\n",
       "6561        0.008403             0           0.000000           17.354167   \n",
       "6562        0.000000             1           0.002381           15.000000   \n",
       "6563        0.002350             2           0.002350           20.261905   \n",
       "6564        0.000000             0           0.000000           14.900000   \n",
       "6566        0.012795             1           0.000492           16.933333   \n",
       "\n",
       "     num_paragraphs  avg_sents_per_paragraph  avg_words_per_paragraph  \\\n",
       "6561             16                 3.187500                50.812500   \n",
       "6562             12                 2.666667                35.000000   \n",
       "6563             14                 3.285714                58.357143   \n",
       "6564              1                10.000000               149.000000   \n",
       "6566             12                 3.750000                67.000000   \n",
       "\n",
       "     num_images num_videos num_youtubes num_hyperlinks num_bolded  \\\n",
       "6561          0          0            0              4          5   \n",
       "6562          0          0            0              1          4   \n",
       "6563          3          0            0              3          9   \n",
       "6564          0          0            0              0          0   \n",
       "6566          3          0            0              3         30   \n",
       "\n",
       "      percent_bolded                                    normalized_text  \n",
       "6561        0.006002  Imagine a world where there are people that ha...  \n",
       "6562        0.009524  Synopsis Set in Orlando, Florida, Blood Child ...  \n",
       "6563        0.010576  Click the image to read about these books The ...  \n",
       "6564        0.000000  I originally planned to quietly publish this n...  \n",
       "6566        0.014764  Thank You! To get in touch with me about my pr...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenating both df_description and  df_description1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_description,df_description1]\n",
    "\n",
    "combined_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5938 entries, 2 to 7883\n",
      "Data columns (total 19 columns):\n",
      "num_sents                  5922 non-null object\n",
      "num_words                  5900 non-null object\n",
      "num_all_caps               5922 non-null object\n",
      "percent_all_caps           5900 non-null float64\n",
      "num_exclms                 5922 non-null object\n",
      "percent_exclms             5900 non-null float64\n",
      "num_imp_words              5922 non-null object\n",
      "percent_imp_words          5900 non-null float64\n",
      "avg_words_per_sent         5902 non-null float64\n",
      "num_paragraphs             5922 non-null object\n",
      "avg_sents_per_paragraph    5832 non-null float64\n",
      "avg_words_per_paragraph    5832 non-null float64\n",
      "num_images                 5922 non-null object\n",
      "num_videos                 5922 non-null object\n",
      "num_youtubes               5922 non-null object\n",
      "num_hyperlinks             5922 non-null object\n",
      "num_bolded                 5922 non-null object\n",
      "percent_bolded             5900 non-null float64\n",
      "normalized_text            5922 non-null object\n",
      "dtypes: float64(7), object(12)\n",
      "memory usage: 927.8+ KB\n"
     ]
    }
   ],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining extracted features from the'project description' with other features from the Web Robots data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Web Robots data contains the target variable, in addition to other interesting features, let's join these data with the extracted meta features and normalized text to complete the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Web Robots data\n",
    "web_robots_data = joblib.load('/Users/shwetapai/Desktop/Project5/Data/testing_1.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the index labels into a new column\n",
    "web_robots_data = web_robots_data.reset_index()\n",
    "\n",
    "combined_df = combined_df.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's join the extracted meta features and normalized text with the Web Robots data, containing the target variable, for each campaign section using only the projects whose features have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Web Robots data with extracted features for each section\n",
    "final_df = combined_df.merge(web_robots_data, how='left', on='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_all_features.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickling the datasets\n",
    "\n",
    "joblib.dump(final_df, 'final_all_features.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling the datasets\n",
    "\n",
    "joblib.dump(section1_merged, 'finalsection1_all_features.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalsection2_all_features.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(section2_merged, 'finalsection2_all_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
